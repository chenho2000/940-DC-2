{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-14T04:15:29.084892Z",
     "start_time": "2024-03-14T04:15:26.852853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setfit\r\n",
      "  Downloading setfit-1.0.3-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: datasets>=2.3.0 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from setfit) (2.12.0)\r\n",
      "Collecting sentence-transformers>=2.2.1 (from setfit)\r\n",
      "  Downloading sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: evaluate>=0.3.0 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from setfit) (0.4.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.13.0 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from setfit) (0.20.3)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from setfit) (1.1.3)\r\n",
      "Requirement already satisfied: packaging in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from setfit) (23.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from datasets>=2.3.0->setfit) (1.26.4)\r\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from datasets>=2.3.0->setfit) (14.0.2)\r\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from datasets>=2.3.0->setfit) (0.3.6)\r\n",
      "Requirement already satisfied: pandas in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from datasets>=2.3.0->setfit) (2.2.1)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from datasets>=2.3.0->setfit) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from datasets>=2.3.0->setfit) (4.65.0)\r\n",
      "Requirement already satisfied: xxhash in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from datasets>=2.3.0->setfit) (2.0.2)\r\n",
      "Requirement already satisfied: multiprocess in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from datasets>=2.3.0->setfit) (0.70.14)\r\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from fsspec[http]>=2021.11.1->datasets>=2.3.0->setfit) (2023.10.0)\r\n",
      "Requirement already satisfied: aiohttp in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from datasets>=2.3.0->setfit) (3.9.3)\r\n",
      "Requirement already satisfied: responses<0.19 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from datasets>=2.3.0->setfit) (0.13.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from datasets>=2.3.0->setfit) (6.0.1)\r\n",
      "Requirement already satisfied: filelock in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from huggingface-hub>=0.13.0->setfit) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from huggingface-hub>=0.13.0->setfit) (4.9.0)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from sentence-transformers>=2.2.1->setfit) (4.37.2)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from sentence-transformers>=2.2.1->setfit) (2.2.0.post100)\r\n",
      "Requirement already satisfied: scipy in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from sentence-transformers>=2.2.1->setfit) (1.10.0)\r\n",
      "Collecting Pillow (from sentence-transformers>=2.2.1->setfit)\r\n",
      "  Using cached pillow-10.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.7 kB)\r\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from scikit-learn->setfit) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from scikit-learn->setfit) (2.2.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from aiohttp->datasets>=2.3.0->setfit) (1.2.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from aiohttp->datasets>=2.3.0->setfit) (23.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from aiohttp->datasets>=2.3.0->setfit) (1.4.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from aiohttp->datasets>=2.3.0->setfit) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from aiohttp->datasets>=2.3.0->setfit) (1.9.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (2.0.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (2024.2.2)\r\n",
      "Requirement already satisfied: six in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from responses<0.19->datasets>=2.3.0->setfit) (1.16.0)\r\n",
      "Requirement already satisfied: sympy in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.1->setfit) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.1->setfit) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.1->setfit) (3.1.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=2.2.1->setfit) (2023.10.3)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=2.2.1->setfit) (0.15.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=2.2.1->setfit) (0.4.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from pandas->datasets>=2.3.0->setfit) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from pandas->datasets>=2.3.0->setfit) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from pandas->datasets>=2.3.0->setfit) (2023.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.1->setfit) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/chen/anaconda3/envs/dev/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.2.1->setfit) (1.3.0)\r\n",
      "Downloading setfit-1.0.3-py3-none-any.whl (75 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m75.9/75.9 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m156.5/156.5 kB\u001B[0m \u001B[31m7.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hUsing cached pillow-10.2.0-cp311-cp311-macosx_11_0_arm64.whl (3.3 MB)\r\n",
      "Installing collected packages: Pillow, sentence-transformers, setfit\r\n",
      "Successfully installed Pillow-10.2.0 sentence-transformers-2.5.1 setfit-1.0.3\r\n"
     ]
    }
   ],
   "source": [
    "! pip install setfit"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import random\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T04:16:26.760725Z",
     "start_time": "2024-03-14T04:16:23.916375Z"
    }
   },
   "id": "32aeed4cd65481",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Check gpu\n",
    "device = \"mps\" if torch.backends.mps.is_built() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T04:17:00.077281Z",
     "start_time": "2024-03-14T04:17:00.073525Z"
    }
   },
   "id": "a561bfc1ebb22ac9",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# For reproducibility, you can define the following function to fix the random seeds.\n",
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "seed_everything(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T04:17:00.443616Z",
     "start_time": "2024-03-14T04:17:00.439357Z"
    }
   },
   "id": "db35cfe28fa3005f",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Read the data\n",
    "yelp_review = pd.read_csv('./data/train.csv', header=0)\n",
    "# make the stars 0, 1, 2\n",
    "yelp_review['stars'] = yelp_review['stars'] - 1\n",
    "\n",
    "test_data = pd.read_csv('./data/new_test.csv', header=0)\n",
    "test_label = list(test_data['stars'])\n",
    "final_test_data = pd.read_csv('./data/test.csv', header=0)\n",
    "final_test_data['ID'] = 0\n",
    "\n",
    "yelp_review = yelp_review.rename(columns={'stars': 'label'})\n",
    "test_data = test_data.rename(columns={'stars': 'label'})\n",
    "final_test_data = final_test_data.rename(columns={'ID': 'label'})\n",
    "\n",
    "dataset = Dataset.from_pandas(yelp_review)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "fina_test_dataset = Dataset.from_pandas(final_test_data)\n",
    "\n",
    "# Load SetFit model from Hub\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T04:17:52.495707Z",
     "start_time": "2024-03-14T04:17:52.410832Z"
    }
   },
   "id": "b699c24fa83cb64e",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7y/z1_q_y7n1rgcftmm9bnt0lt00000gn/T/ipykernel_2972/4267404778.py:2: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "  trainer = SetFitTrainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05158c594a274366874bed51fd4aa288"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create trainer\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    batch_size=4,\n",
    "    num_iterations=20, # Number of text pairs to generate for contrastive learning\n",
    "    num_epochs=1 # Number of epochs to use for contrastive learning\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T04:23:21.853619Z",
     "start_time": "2024-03-14T04:23:21.704462Z"
    }
   },
   "id": "5d01ea721018ee18",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 240000\n",
      "  Batch size = 4\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 60000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='60000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    2/60000 : < :, Epoch 0.00/0]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Train and evaluate!\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m      3\u001B[0m metrics \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mevaluate()\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/setfit/trainer.py:410\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, args, trial, **kwargs)\u001B[0m\n\u001B[1;32m    405\u001B[0m train_parameters \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset_to_parameters(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_dataset)\n\u001B[1;32m    406\u001B[0m full_parameters \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    407\u001B[0m     train_parameters \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset_to_parameters(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meval_dataset) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meval_dataset \u001B[38;5;28;01melse\u001B[39;00m train_parameters\n\u001B[1;32m    408\u001B[0m )\n\u001B[0;32m--> 410\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_embeddings(\u001B[38;5;241m*\u001B[39mfull_parameters, args\u001B[38;5;241m=\u001B[39margs)\n\u001B[1;32m    411\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_classifier(\u001B[38;5;241m*\u001B[39mtrain_parameters, args\u001B[38;5;241m=\u001B[39margs)\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/setfit/trainer.py:462\u001B[0m, in \u001B[0;36mTrainer.train_embeddings\u001B[0;34m(self, x_train, y_train, x_eval, y_eval, args)\u001B[0m\n\u001B[1;32m    459\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  Total optimization steps = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_train_steps\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    461\u001B[0m warmup_steps \u001B[38;5;241m=\u001B[39m math\u001B[38;5;241m.\u001B[39mceil(total_train_steps \u001B[38;5;241m*\u001B[39m args\u001B[38;5;241m.\u001B[39mwarmup_proportion)\n\u001B[0;32m--> 462\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_sentence_transformer(\n\u001B[1;32m    463\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mmodel_body,\n\u001B[1;32m    464\u001B[0m     train_dataloader\u001B[38;5;241m=\u001B[39mtrain_dataloader,\n\u001B[1;32m    465\u001B[0m     eval_dataloader\u001B[38;5;241m=\u001B[39meval_dataloader,\n\u001B[1;32m    466\u001B[0m     args\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m    467\u001B[0m     loss_func\u001B[38;5;241m=\u001B[39mloss_func,\n\u001B[1;32m    468\u001B[0m     warmup_steps\u001B[38;5;241m=\u001B[39mwarmup_steps,\n\u001B[1;32m    469\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/setfit/trainer.py:641\u001B[0m, in \u001B[0;36mTrainer._train_sentence_transformer\u001B[0;34m(self, model_body, train_dataloader, eval_dataloader, args, loss_func, warmup_steps)\u001B[0m\n\u001B[1;32m    639\u001B[0m     skip_scheduler \u001B[38;5;241m=\u001B[39m scaler\u001B[38;5;241m.\u001B[39mget_scale() \u001B[38;5;241m!=\u001B[39m scale_before_step\n\u001B[1;32m    640\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 641\u001B[0m     loss_value \u001B[38;5;241m=\u001B[39m loss_func(features, labels)\n\u001B[1;32m    642\u001B[0m     loss_value\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m    643\u001B[0m     torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(loss_func\u001B[38;5;241m.\u001B[39mparameters(), max_grad_norm)\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/sentence_transformers/losses/CosineSimilarityLoss.py:63\u001B[0m, in \u001B[0;36mCosineSimilarityLoss.forward\u001B[0;34m(self, sentence_features, labels)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, sentence_features: Iterable[Dict[\u001B[38;5;28mstr\u001B[39m, Tensor]], labels: Tensor):\n\u001B[0;32m---> 63\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(sentence_feature)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentence_embedding\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m sentence_feature \u001B[38;5;129;01min\u001B[39;00m sentence_features]\n\u001B[1;32m     64\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcos_score_transformation(torch\u001B[38;5;241m.\u001B[39mcosine_similarity(embeddings[\u001B[38;5;241m0\u001B[39m], embeddings[\u001B[38;5;241m1\u001B[39m]))\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fct(output, labels\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/sentence_transformers/losses/CosineSimilarityLoss.py:63\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, sentence_features: Iterable[Dict[\u001B[38;5;28mstr\u001B[39m, Tensor]], labels: Tensor):\n\u001B[0;32m---> 63\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(sentence_feature)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentence_embedding\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m sentence_feature \u001B[38;5;129;01min\u001B[39;00m sentence_features]\n\u001B[1;32m     64\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcos_score_transformation(torch\u001B[38;5;241m.\u001B[39mcosine_similarity(embeddings[\u001B[38;5;241m0\u001B[39m], embeddings[\u001B[38;5;241m1\u001B[39m]))\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fct(output, labels\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:98\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[0;34m(self, features)\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m features:\n\u001B[1;32m     96\u001B[0m     trans_features[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m features[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m---> 98\u001B[0m output_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtrans_features, return_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     99\u001B[0m output_tokens \u001B[38;5;241m=\u001B[39m output_states[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    101\u001B[0m features\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken_embeddings\u001B[39m\u001B[38;5;124m\"\u001B[39m: output_tokens, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m: features[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m]})\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py:551\u001B[0m, in \u001B[0;36mMPNetModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001B[0m\n\u001B[1;32m    549\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m    550\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(input_ids\u001B[38;5;241m=\u001B[39minput_ids, position_ids\u001B[38;5;241m=\u001B[39mposition_ids, inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds)\n\u001B[0;32m--> 551\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\n\u001B[1;32m    552\u001B[0m     embedding_output,\n\u001B[1;32m    553\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mextended_attention_mask,\n\u001B[1;32m    554\u001B[0m     head_mask\u001B[38;5;241m=\u001B[39mhead_mask,\n\u001B[1;32m    555\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m    556\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[1;32m    557\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[1;32m    558\u001B[0m )\n\u001B[1;32m    559\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    560\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py:341\u001B[0m, in \u001B[0;36mMPNetEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001B[0m\n\u001B[1;32m    338\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states:\n\u001B[1;32m    339\u001B[0m     all_hidden_states \u001B[38;5;241m=\u001B[39m all_hidden_states \u001B[38;5;241m+\u001B[39m (hidden_states,)\n\u001B[0;32m--> 341\u001B[0m layer_outputs \u001B[38;5;241m=\u001B[39m layer_module(\n\u001B[1;32m    342\u001B[0m     hidden_states,\n\u001B[1;32m    343\u001B[0m     attention_mask,\n\u001B[1;32m    344\u001B[0m     head_mask[i],\n\u001B[1;32m    345\u001B[0m     position_bias,\n\u001B[1;32m    346\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m    347\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    348\u001B[0m )\n\u001B[1;32m    349\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    351\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py:300\u001B[0m, in \u001B[0;36mMPNetLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001B[0m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    292\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    293\u001B[0m     hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    298\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    299\u001B[0m ):\n\u001B[0;32m--> 300\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention(\n\u001B[1;32m    301\u001B[0m         hidden_states,\n\u001B[1;32m    302\u001B[0m         attention_mask,\n\u001B[1;32m    303\u001B[0m         head_mask,\n\u001B[1;32m    304\u001B[0m         position_bias\u001B[38;5;241m=\u001B[39mposition_bias,\n\u001B[1;32m    305\u001B[0m         output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m    306\u001B[0m     )\n\u001B[1;32m    307\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    308\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add self attentions if we output attention weights\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py:241\u001B[0m, in \u001B[0;36mMPNetAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    234\u001B[0m     hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    240\u001B[0m ):\n\u001B[0;32m--> 241\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn(\n\u001B[1;32m    242\u001B[0m         hidden_states,\n\u001B[1;32m    243\u001B[0m         attention_mask,\n\u001B[1;32m    244\u001B[0m         head_mask,\n\u001B[1;32m    245\u001B[0m         position_bias,\n\u001B[1;32m    246\u001B[0m         output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m    247\u001B[0m     )\n\u001B[1;32m    248\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLayerNorm(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(self_outputs[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m+\u001B[39m hidden_states)\n\u001B[1;32m    249\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py:188\u001B[0m, in \u001B[0;36mMPNetSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001B[0m\n\u001B[1;32m    185\u001B[0m     attention_scores \u001B[38;5;241m=\u001B[39m attention_scores \u001B[38;5;241m+\u001B[39m attention_mask\n\u001B[1;32m    187\u001B[0m \u001B[38;5;66;03m# Normalize the attention scores to probabilities.\u001B[39;00m\n\u001B[0;32m--> 188\u001B[0m attention_probs \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39msoftmax(attention_scores, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    190\u001B[0m attention_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(attention_probs)\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m head_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/dev/lib/python3.11/site-packages/torch/nn/functional.py:1828\u001B[0m, in \u001B[0;36msoftmax\u001B[0;34m(input, dim, _stacklevel, dtype)\u001B[0m\n\u001B[1;32m   1824\u001B[0m         ret \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m-\u001B[39m\u001B[38;5;28minput\u001B[39m)\u001B[38;5;241m.\u001B[39msoftmax(dim, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[1;32m   1825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ret\n\u001B[0;32m-> 1828\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msoftmax\u001B[39m(\u001B[38;5;28minput\u001B[39m: Tensor, dim: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, _stacklevel: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m, dtype: Optional[DType] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m   1829\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Apply a softmax function.\u001B[39;00m\n\u001B[1;32m   1830\u001B[0m \n\u001B[1;32m   1831\u001B[0m \u001B[38;5;124;03m    Softmax is defined as:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1851\u001B[0m \n\u001B[1;32m   1852\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   1853\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28minput\u001B[39m):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Train and evaluate!\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T04:26:25.909612Z",
     "start_time": "2024-03-14T04:23:24.129466Z"
    }
   },
   "id": "b1dd1eb719b6ea8d",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fee8b36693dce3fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
